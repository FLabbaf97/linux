diff -uNr linux-4.9.20_origin/kernel/sched/core.c linux-4.9.20/kernel/sched/core.c
--- linux-4.9.20_origin/kernel/sched/core.c	2017-03-31 13:02:02.000000000 +0430
+++ linux-4.9.20/kernel/sched/core.c	2018-01-22 19:16:21.783878000 +0330
@@ -2268,7 +2268,7 @@
 int sysctl_numa_balancing(struct ctl_table *table, int write,
 			 void __user *buffer, size_t *lenp, loff_t *ppos)
 {
-	struct ctl_table t;
+	ctl_table_no_const t;
 	int err;
 	int state = static_branch_likely(&sched_numa_balancing);
 
@@ -2343,7 +2343,7 @@
 int sysctl_schedstats(struct ctl_table *table, int write,
 			 void __user *buffer, size_t *lenp, loff_t *ppos)
 {
-	struct ctl_table t;
+	ctl_table_no_const t;
 	int err;
 	int state = static_branch_likely(&sched_schedstats);
 
@@ -2383,7 +2383,18 @@
 	/*
 	 * Make sure we do not leak PI boosting priority to the child.
 	 */
-	p->prio = current->normal_prio;
+
+	 //farzaneh labbaf: force scheduler to use round robin or FCFS
+	 if (p->policy == SCHED_NORMAL) {
+         p->prio = current->normal_prio - NICE_WIDTH -
+                  PRIO_TO_NICE(current->static_prio);
+         p->normal_prio = p->prio;
+       	 p->rt_priority = p->prio;
+         p->policy = SCHED_RR;
+				 /*replace line above with line below if youre using FCFS */
+				 //p->policy = SCHED_RR;
+         p->static_prio = NICE_TO_PRIO(0);
+   }
 
 	/*
 	 * Revert to default priority/policy on fork if requested.
@@ -2797,7 +2808,7 @@
 /* rq->lock is NOT held, but preemption is disabled */
 static void __balance_callback(struct rq *rq)
 {
-	struct callback_head *head, *next;
+	struct balance_callback *head, *next;
 	void (*func)(struct rq *rq);
 	unsigned long flags;
 
@@ -2805,7 +2816,7 @@
 	head = rq->balance_callback;
 	rq->balance_callback = NULL;
 	while (head) {
-		func = (void (*)(struct rq *))head->func;
+		func = head->func;
 		next = head->next;
 		head->next = NULL;
 		head = next;
@@ -3795,6 +3806,8 @@
 	/* convert nice value [19,-20] to rlimit style value [1,40] */
 	int nice_rlim = nice_to_rlimit(nice);
 
+	gr_learn_resource(p, RLIMIT_NICE, nice_rlim, 1);
+
 	return (nice_rlim <= task_rlimit(p, RLIMIT_NICE) ||
 		capable(CAP_SYS_NICE));
 }
@@ -3821,7 +3834,8 @@
 	nice = task_nice(current) + increment;
 
 	nice = clamp_val(nice, MIN_NICE, MAX_NICE);
-	if (increment < 0 && !can_nice(current, nice))
+	if (increment < 0 && (!can_nice(current, nice) ||
+			      gr_handle_chroot_nice()))
 		return -EPERM;
 
 	retval = security_task_setnice(current, nice);
@@ -4084,6 +4098,13 @@
 	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE;
 	struct rq *rq;
 
+
+/* farzaneh labbaf: to force using round robin or FCFS scheduler*/
+	if (attr.sched_policy == SCHED_NORMAL) {
+         attr.sched_priority = param->sched_priority -
+                 NICE_WIDTH - attr.sched_nice;
+         attr.sched_policy = SCHED_RR;
+  }
 	/* may grab non-irq protected spin_locks */
 	BUG_ON(in_interrupt());
 recheck:
@@ -4131,6 +4152,7 @@
 			if (policy != p->policy && !rlim_rtprio)
 				return -EPERM;
 
+			gr_learn_resource(p, RLIMIT_RTPRIO, attr->sched_priority, 1);
 			/* can't increase priority */
 			if (attr->sched_priority > p->rt_priority &&
 			    attr->sched_priority > rlim_rtprio)
@@ -7591,6 +7613,30 @@
 	for_each_possible_cpu(i) {
 		struct rq *rq;
 
+#if defined(CONFIG_GRKERNSEC_KSTACKOVERFLOW) && defined(CONFIG_X86_64)
+		struct page *newstack_page = alloc_pages_node(cpu_to_node(i), GFP_KERNEL|__GFP_NOTRACK|__GFP_ZERO, IRQ_STACK_ORDER);
+		void *newstack_lowmem = NULL;
+		void *newstack;
+		struct page *pages[IRQ_STACK_SIZE / PAGE_SIZE];
+		unsigned int x;
+
+		if (newstack_page)
+			newstack_lowmem = page_address(newstack_page);
+
+		if (newstack_lowmem == NULL)
+			panic("grsec: Unable to allocate irq stack");
+
+		for (x = 0; x < IRQ_STACK_SIZE / PAGE_SIZE; x++)
+			pages[x] = virt_to_page(newstack_lowmem + (x * PAGE_SIZE));
+
+		newstack = vmap(pages, IRQ_STACK_SIZE / PAGE_SIZE, VM_IOREMAP, PAGE_KERNEL);
+		if (newstack == NULL)
+			panic("grsec: Unable to vmap irq stack");
+		populate_stack(newstack, IRQ_STACK_SIZE);
+		per_cpu(irq_stack_ptr_lowmem, i) = newstack_lowmem + IRQ_STACK_SIZE - 64;
+		per_cpu(irq_stack_ptr, i) = newstack + IRQ_STACK_SIZE - 64;
+#endif
+
 		rq = cpu_rq(i);
 		raw_spin_lock_init(&rq->lock);
 		rq->nr_running = 0;
@@ -7712,7 +7758,7 @@
 	 */
 	WARN_ONCE(current->state != TASK_RUNNING && current->task_state_change,
 			"do not call blocking ops when !TASK_RUNNING; "
-			"state=%lx set at [<%p>] %pS\n",
+			"state=%lx set at [<%p>] %pA\n",
 			current->state,
 			(void *)current->task_state_change,
 			(void *)current->task_state_change);
